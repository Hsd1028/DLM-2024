# 关于激活函数

## 一、Relu( )
ReLU，即Rectified Linear Unit，是一种用于人工神经网络的激活函数。它在输入为正时返回输入本身，而在输入为负时返回 0。数学上，ReLU函数可以表示为：
f(x)=max(0,x)

其中，x 是输入。ReLU函数的图像是一条从原点延伸的直线，当 x 大于等于0时，斜率为1，而当 x 小于0时，函数值为0。

用处：

1、解决梯度消失问题：在反向传播算法中，ReLU的导数在输入为正时恒为1，这有助于解决梯度消失问题，使得神经网络的训练更加稳定。

2、计算简单：ReLU函数的计算非常简单，只需判断输入是否大于0即可，因此在实际应用中计算效率较高。

3、稀疏激活性：在ReLU中，大部分神经元会保持静止，即输出为0，这样的特性有助于模型的稀疏性，从而减少了参数的相关性，降低了过拟合的风险。

缺点：

1、“神经元死亡”的问题，即当某个神经元的权重更新使得其始终输出为0时，这个神经元在训练过程中将不再起作用。

2、在负数部分输出为0可能会导致一些神经元“坏死”，因为它们在训练过程中不再更新。

## 二、sigmoid( )
Sigmoid函数是一种常用的激活函数，它将实数映射到一个范围在0到1之间的值。数学上，Sigmoid函数可以表示为：
f(x)=1/(1+e^(-x))

其中， x 是输入。Sigmoid函数的图像是一条S形曲线，随着输入的增大，输出趋于1；随着输入的减小，输出趋于0。

用处：

1、输出范围有界：Sigmoid函数的输出范围在0到1之间，这使得它特别适合用于输出层的激活函数，用来表示分类问题中的概率值。

2、平滑性：Sigmoid函数在整个实数范围内都是可导的，而且具有连续性和平滑性，这有助于在反向传播算法中进行梯度计算。

3、非线性：Sigmoid函数是一种非线性函数，这使得神经网络能够学习和逼近复杂的非线性关系，提高了神经网络的表达能力。

缺点：

1、梯度消失问题：在Sigmoid函数的两端，梯度接近于0，这会导致在深层神经网络中出现梯度消失问题，使得反向传播时底层网络的参数很难得到有效的更新。

2、计算代价高：Sigmoid函数的计算相对复杂，涉及指数运算，因此在大规模神经网络中的计算代价较高。
